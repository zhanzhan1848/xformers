Search.setIndex({"docnames": ["components/attentions", "components/feedforward", "components/index", "components/mha", "components/ops", "components/position_embedding", "components/reversible", "custom_parts/index", "index", "tutorials/extend_attentions", "tutorials/index", "tutorials/reversible", "tutorials/sparse_vit", "tutorials/use_attention", "what_is_xformers"], "filenames": ["components/attentions.rst", "components/feedforward.rst", "components/index.rst", "components/mha.rst", "components/ops.rst", "components/position_embedding.rst", "components/reversible.rst", "custom_parts/index.rst", "index.rst", "tutorials/extend_attentions.rst", "tutorials/index.rst", "tutorials/reversible.rst", "tutorials/sparse_vit.rst", "tutorials/use_attention.rst", "what_is_xformers.rst"], "titles": ["Attention mechanisms", "Feedforward mechanisms", "API Reference", "Multi Head Attention", "xFormers optimized operators", "Position Embeddings", "Reversible layer", "Custom parts reference", "Welcome to xFormers\u2019s documentation!", "Extend the xFormers parts zoo", "Tutorials", "Using the Reversible block", "Replace all attentions from an existing ViT model with a sparse equivalent?", "I\u2019m only interested in testing out the attention mechanisms that are hosted here", "What is xFormers?"], "terms": {"class": [0, 1, 3, 4, 5, 6, 9, 11], "xformer": [0, 1, 2, 3, 5, 6, 7, 10, 11, 12, 13], "compon": [0, 1, 3, 5, 6, 9, 11, 12, 13], "scaleddotproduct": [0, 12], "dropout": [0, 1, 3, 4, 5, 9, 13], "float": [0, 1, 3, 4, 5, 9], "0": [0, 3, 4, 5, 7, 13], "causal": [0, 4, 9], "bool": [0, 1, 3, 4, 5, 6, 9, 11], "fals": [0, 3, 6, 9, 11, 12], "seq_len": [0, 5, 13], "option": [0, 1, 3, 4, 8, 9, 11], "int": [0, 1, 3, 4, 5, 6, 9, 11], "none": [0, 1, 3, 4, 9], "to_seq_len": 0, "arg": [0, 1, 3, 5, 6, 9], "kwarg": [0, 1, 3, 4, 5, 6, 9, 11], "sourc": [0, 1, 3, 4, 5, 6, 7, 12, 14], "base": [0, 1, 4, 5, 7, 8, 9, 12, 13], "implement": [0, 5, 7, 11], "scale": [0, 4, 7], "dot": [0, 7], "product": [0, 7], "propos": [0, 3, 11, 12, 14], "all": [0, 3, 4, 8, 9, 10, 11, 13, 14], "you": [0, 3, 4, 7, 9, 11, 12, 13, 14], "need": [0, 3, 4, 9, 11], "vaswani": [0, 3, 11], "et": [0, 3, 5, 11], "al": [0, 3, 5, 11], "mask": [0, 3, 4, 7, 12, 13], "attentionmask": 0, "forward": [0, 1, 3, 4, 5, 6, 9, 11], "q": [0, 3, 4, 5, 9], "tensor": [0, 1, 3, 4, 5, 6, 9, 11, 12], "k": [0, 3, 4, 5, 9], "v": [0, 3, 4, 9], "att_mask": [0, 3, 12], "union": [0, 1, 4, 5, 11], "A": [0, 3, 4, 5, 7, 11], "2d": 0, "3d": 0, "which": [0, 1, 4, 5, 7, 8, 9, 11, 12, 13, 14], "ignor": [0, 4], "certain": 0, "posit": [0, 2, 4, 8], "If": [0, 4], "boolean": [0, 11], "valu": [0, 3, 4, 7, 14], "true": [0, 1, 3, 4, 6, 7, 9, 11], "keep": 0, "while": 0, "kei": [0, 1, 3, 4, 5], "pad": [0, 4], "dimens": [0, 3, 4, 13], "batch": [0, 3, 4, 9, 13], "x": [0, 4, 5, 6, 7, 9, 11, 13], "sequenc": [0, 3, 4, 11, 13], "length": [0, 3, 4], "OR": 0, "can": [0, 3, 4, 7, 8, 9, 11, 12, 13, 14], "combin": [0, 4, 8, 11, 13], "pass": [0, 4, 11], "here": [0, 4, 8, 10], "method": [0, 5], "maybe_merge_mask": 0, "provid": [0, 3, 4, 7, 13], "util": 0, "us": [0, 1, 3, 4, 5, 7, 8, 9, 10, 12, 13], "merg": [0, 4], "ha": [0, 4], "type": [0, 1, 4, 5, 9], "an": [0, 4, 5, 8, 10, 13], "addit": [0, 4], "expect": [0, 3, 13], "ar": [0, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14], "inf": [0, 4], "localattent": 0, "window_s": [0, 4], "5": [0, 4], "force_spars": 0, "__init__": [0, 9, 11], "slide": 0, "window": [0, 4], "routingtransform": 0, "longform": 0, "bigbird": 0, "paramet": [0, 3, 4, 12, 13], "probabl": [0, 4], "output": [0, 3, 4, 11, 12], "randomli": 0, "drop": 0, "train": [0, 1, 3, 5, 6], "time": 0, "appli": [0, 4, 9, 11], "cannot": [0, 4, 14], "futur": 0, "overal": 0, "size": [0, 3, 4], "local": [0, 4, 9, 14], "odd": 0, "number": [0, 3, 4, 11], "evenli": 0, "distribut": [0, 11], "both": [0, 4, 9], "side": 0, "each": [0, 4, 14], "queri": [0, 3, 4, 5], "linformerattent": 0, "linform": 0, "from": [0, 1, 4, 5, 7, 8, 9, 10, 11, 13], "self": [0, 1, 3, 4, 9, 11], "linear": [0, 4], "complex": 0, "wang": 0, "2020": [0, 11], "The": [0, 3, 4, 5, 7, 9, 11, 12, 13], "origin": [0, 11], "notat": 0, "kept": 0, "nystromattent": [0, 9], "num_head": [0, 3, 4, 9, 12, 13], "num_landmark": [0, 9], "64": [0, 9], "landmark_pool": [0, 9], "modul": [0, 1, 3, 5, 6, 7, 9, 11, 12], "use_razavi_pinvers": [0, 9], "pinverse_original_init": [0, 9], "inv_iter": [0, 9], "6": [0, 4, 7, 9], "v_skip_connect": [0, 9], "conv_kernel_s": [0, 9], "nystrom": [0, 9], "nystromform": 0, "algorithm": 0, "approxim": 0, "xiong": 0, "y": [0, 4, 6], "zeng": 0, "z": 0, "chakraborti": 0, "r": [0, 7, 11], "tan": 0, "m": [0, 4, 8, 10, 11], "fung": 0, "g": [0, 4, 6, 11], "li": 0, "singh": 0, "2021": 0, "refer": [0, 8, 12, 14], "codebas": 0, "http": [0, 4], "github": [0, 7], "com": [0, 7], "mlpen": 0, "key_padding_mask": [0, 3], "onli": [0, 4, 8, 10], "accept": [0, 9], "must": [0, 4], "1": [0, 4, 5, 6, 11, 12, 13], "correct": 0, "mean": [0, 11, 14], "randomattent": 0, "01": 0, "constant_mask": 0, "random": 0, "instanc": [0, 1, 5, 9], "case": [0, 4, 12, 13], "attend": [0, 4], "set": [0, 4, 7, 11], "thi": [0, 1, 3, 4, 5, 9, 11, 12, 13, 14], "spars": [0, 8, 10], "awar": [0, 3, 12], "empti": [0, 4], "part": [0, 1, 4, 5, 10], "repres": 0, "memori": [0, 2, 11], "ratio": 0, "same": [0, 4, 9, 11], "orthoformerattent": 0, "32": [0, 4], "subsample_fract": 0, "landmark_select": 0, "landmarkselect": 0, "orthogon": 0, "orthoform": 0, "your": [0, 9, 11, 13], "ey": 0, "ball": 0, "trajectori": 0, "video": 0, "transform": [0, 5, 8, 12, 14], "patrick": 0, "campbel": 0, "d": [0, 4], "asano": 0, "misra": 0, "i": [0, 4, 8, 10], "metz": 0, "f": [0, 4, 6, 11], "feichtenhof": 0, "c": [0, 9], "vedaldi": 0, "henriqu": 0, "j": 0, "facebookresearch": 0, "motionform": 0, "globalattent": 0, "attention_query_mask": [0, 13], "_": [0, 5], "__": [0, 5], "global": 0, "label": 0, "other": [0, 4, 5, 9, 11, 12, 14], "neg": [0, 4], "ani": [0, 1, 4, 5, 9, 12, 14], "element": [0, 4], "zero": [0, 4], "torch": [0, 4, 9, 11, 12, 13], "favorattent": 0, "dim_featur": 0, "dim_head": 0, "iter_before_redraw": 0, "feature_map_typ": 0, "featuremaptyp": 0, "smreg": 0, "normalize_input": 0, "kernel": [0, 4, 8], "perform": [0, 4], "rethink": 0, "choromanski": 0, "favor": 0, "stand": 0, "fast": [0, 4, 14], "via": [0, 4, 11], "featur": [0, 4], "space": [0, 4], "step": 0, "call": [0, 1, 4, 5, 9], "befor": [0, 4], "redraw": 0, "map": 0, "being": [0, 3, 14], "typic": [0, 12], "sub": 0, "multi": [0, 2, 4, 8, 11, 13], "head": [0, 2, 4, 8, 11, 13], "classmethod": [0, 1, 3, 4], "from_config": [0, 1, 3, 5], "config": [0, 1, 3, 5, 9], "attentionconfig": [0, 9], "abstract": 0, "additive_mask": 0, "is_caus": 0, "object": [0, 4], "hold": 0, "along": [0, 4], "coupl": [0, 7, 9], "helper": [0, 11, 12, 13], "attribut": 0, "to_bool": 0, "from_bool": 0, "creat": [0, 4, 7, 8], "given": [0, 1, 4, 5, 9, 12, 13, 14], "pattern": [0, 4, 12], "warn": [0, 4], "we": [0, 4, 9, 11, 12, 13], "assum": [0, 1, 4, 5], "impli": 0, "should": [0, 4, 11, 13], "comput": [0, 4, 7, 11], "from_multipl": 0, "multipl": [0, 11], "make_caus": [0, 4], "devic": [0, 4, 13], "dtype": [0, 4], "make_crop": 0, "return": [0, 4, 12], "crop": 0, "whose": [0, 4], "underli": 0, "view": 0, "one": [0, 4, 9, 11, 12, 14], "properti": 0, "is_spars": 0, "ndim": 0, "shape": [0, 4, 12], "build_attent": [0, 13], "dict": [0, 1, 4, 5, 13], "str": [0, 1, 4, 5], "build": [0, 1, 5, 8, 11, 13, 14], "name": [0, 1, 5, 7, 9, 12, 13], "determin": [0, 1, 5], "what": [0, 1, 5], "instanti": [0, 1, 5, 13], "For": [0, 1, 4, 5], "my_attent": 0, "foo": [0, 1, 5], "bar": [0, 1, 5], "find": [0, 1, 5], "wa": [0, 1, 5, 9, 11, 13], "regist": [0, 1, 3, 4, 5, 9], "see": [0, 1, 4, 5, 7, 11], "register_attent": [0, 9], "subclass": [0, 1, 5], "decor": [0, 1, 5], "allow": [0, 1, 4, 5, 13, 14], "configur": [0, 1, 5, 9], "file": [0, 1, 4, 5, 13], "even": [0, 1, 5, 9], "itself": [0, 1, 5], "librari": [0, 1, 3, 5, 8, 14], "mlp": [1, 11], "dim_model": [1, 3, 5, 13], "activ": [1, 7, 11], "hidden_layer_multipli": 1, "bia": [1, 3, 4], "input": [1, 3, 4, 11, 13], "feedforwardconfig": 1, "build_feedforward": 1, "attent": [1, 2, 5, 7, 8, 9, 10, 11], "my_feedforward": 1, "register_feedforward": 1, "optim": [2, 8, 14], "oper": [2, 8], "effici": [2, 11], "mechan": [2, 3, 4, 8, 9, 10, 12], "feedforward": [2, 8, 11], "embed": [2, 3, 4, 8], "revers": [2, 8, 10], "layer": [2, 8, 11], "multiheaddispatch": [3, 13], "tupl": [3, 4, 5], "residual_dropout": [3, 13], "use_separate_proj_weight": 3, "dim_kei": 3, "dim_valu": 3, "in_proj_contain": 3, "inputproject": 3, "use_rotary_embed": 3, "out_proj": 3, "dispatch": [3, 4, 13], "project": 3, "end": 3, "follow": [3, 4, 9, 11, 12, 14], "architectur": [3, 9, 13, 14], "actual": [3, 4], "vari": [3, 4], "well": [3, 12], "wrap": [3, 11], "make": [3, 4, 11, 13], "them": [3, 9, 11], "model": [3, 8, 10, 11, 13, 14], "whether": [3, 4, 11, 13], "amount": 3, "residu": [3, 11], "path": [3, 9, 11, 13], "differ": [3, 4, 9, 11], "weight": [3, 12], "rotari": [3, 5], "emb": 3, "dim": [3, 4, 11, 12], "multiheaddispatchconfig": 3, "op": 4, "attentionopbas": 4, "baseoper": 4, "fmha": 4, "cutlass": 4, "fwop": 4, "bwop": 4, "flash": 4, "triton": 4, "not_supported_reason": 4, "list": 4, "reason": 4, "why": 4, "support": [4, 7, 12], "run": 4, "memory_efficient_attent": 4, "attn_bia": 4, "attentionbia": 4, "p": 4, "attentionfwopbas": 4, "attentionbwopbas": 4, "output_dtyp": 4, "doe": [4, 11], "Not": 4, "o": 4, "n": [4, 9, 11], "2": [4, 11], "format": 4, "b": [4, 11], "h": 4, "where": 4, "per": 4, "have": [4, 12], "3": [4, 7, 11, 12, 13], "also": [4, 7, 9, 11, 14], "gqa": 4, "note": [4, 11, 12], "below": 4, "contigu": 4, "requir": [4, 7, 9, 11], "last": 4, "s": [4, 9, 12, 13], "stride": 4, "equival": [4, 8, 10], "pytorch": [4, 8, 12], "code": [4, 7, 9, 13], "transpos": 4, "attn": 4, "softmax": 4, "exampl": [4, 7, 12], "import": [4, 12, 13], "xop": 4, "regular": 4, "With": 4, "lowertriangularmask": 4, "hardwar": 4, "nvidia": 4, "gpu": [4, 11], "capabl": [4, 7], "abov": [4, 11, 12], "p100": 4, "datatyp": 4, "f16": 4, "bf16": 4, "f32": 4, "experiment": 4, "mqa": 4, "group": 4, "16": [4, 12, 13], "8": [4, 7, 12], "pleas": [4, 11, 12], "automat": [4, 7, 9, 14], "broadcast": 4, "so": [4, 9, 14], "manual": [4, 12], "128": 4, "cuda": [4, 8, 12], "float16": 4, "randn": 4, "out_gqa": 4, "reshap": 4, "4": 4, "expand": 4, "rais": 4, "notimplementederror": 4, "mha": [4, 11], "valueerror": 4, "invalid": 4, "mq": 4, "mkv": 4, "kv": 4, "matrix": 4, "default": [4, 9], "common": [4, 7], "arbitrari": 4, "slower": 4, "disabl": 4, "factor": 4, "recommend": [4, 9], "best": 4, "depend": [4, 5, 13], "larg": [4, 11], "includ": [4, 11], "without": [4, 11], "tensorcor": 4, "old": 4, "sm60": 4, "ck": [4, 9], "compos": [4, 14], "ck_decod": 4, "256": 4, "fit": 4, "test": [4, 8, 9, 10, 14], "work": [4, 9, 12, 13], "mi250x": 4, "contain": 4, "argument": [4, 9], "essenti": 4, "ad": 4, "t": 4, "goal": 4, "custom": 4, "made": [4, 11], "instead": 4, "dens": 4, "want": [4, 7, 9, 12], "avoid": 4, "load": [4, 7], "abl": [4, 7], "know": 4, "hand": 4, "eg": 4, "some": [4, 7, 11, 12, 13], "veri": [4, 11, 13], "blockdiagonalmask": 4, "That": [4, 13], "function": [4, 11, 12], "abil": 4, "add": 4, "qk": 4, "calcul": 4, "n_queri": 4, "most": 4, "infin": 4, "form": 4, "children": 4, "defin": [4, 9, 11, 12, 14], "altern": [4, 12, 13], "thing": [4, 9], "when": [4, 7, 11, 12], "materi": 4, "hardcod": 4, "better": 4, "lowertriangularfrombottomrightmask": 4, "lowertriangularmaskwithtensorbia": 4, "blockdiagonalcausalmask": 4, "float32": 4, "cpu": 4, "slow": [4, 7], "don": 4, "attempt": 4, "debug": 4, "like": [4, 9, 11, 12, 13], "q_seqlen": 4, "k_seqlen": 4, "localattentionfrombottomrightmask": 4, "window_left": 4, "window_right": 4, "_left": 4, "_right": 4, "num": 4, "_queri": 4, "_kei": 4, "print": 4, "exp": 4, "4x4": 4, "4x5": 4, "illustr": 4, "total": 4, "exactli": [4, 11], "triangular": 4, "shift": 4, "In": [4, 12, 13], "word": 4, "nearer": 4, "final": 4, "than": [4, 7, 11], "between": [4, 11], "left": 4, "right": 4, "thei": [4, 8], "becom": [4, 11, 13], "equal": 4, "make_local_attent": 4, "lowertriangularfrombottomrightlocalattentionmask": 4, "new": [4, 9, 13, 14], "_window_s": 4, "distanc": 4, "either": [4, 13], "less": [4, 7, 11], "e": [4, 7], "greater": 4, "green": 4, "area": 4, "grei": 4, "out": [4, 8, 10], "q_seqinfo": 4, "_seqleninfo": 4, "k_seqinfo": 4, "_batch_siz": 4, "block": [4, 6, 8, 9, 10, 14], "diagon": 4, "divid": 4, "handl": 4, "from_tensor_list": 4, "list_x": 4, "nn": [4, 9, 11, 12], "unbind": 4, "list_out": 4, "split": 4, "assert": 4, "from_seqlen": 4, "kv_seqlen": 4, "concaten": 4, "back": 4, "m_i": 4, "correspond": 4, "sum_i": 4, "invers": 4, "token": 4, "possibl": [4, 7, 13], "make_causal_from_bottomright": 4, "blockdiagonalcausalfrombottomrightmask": 4, "prefix": 4, "blockdiagonalcausallocalattentionmask": 4, "make_local_attention_from_bottomright": 4, "blockdiagonalcausallocalattentionfrombottomrightmask": 4, "start": 4, "bottom": 4, "except": 4, "nor": 4, "farther": 4, "initi": [4, 13], "num_kei": 4, "num_queri": 4, "otherwis": 4, "vector": 4, "blockdiagonalpaddedkeysmask": 4, "_paddedseqleninfo": 4, "12": 4, "three": [4, 9], "max": 4, "first": [4, 11], "kv_pad": 4, "causal_diagon": 4, "upperbound": 4, "individu": 4, "unus": 4, "bc": 4, "blockdiagonalcausalwithoffsetpaddedkeysmask": 4, "offset": 4, "blockdiagonalcausallocalattentionpaddedkeysmask": 4, "more": [4, 9, 11, 12, 13, 14], "further": 4, "pagedblockdiagonalpaddedkeysmask": 4, "block_tabl": 4, "page_s": 4, "page": 4, "batch_siz": 4, "max_num_pag": 4, "head_dim": 4, "num_group": 4, "pagedblockdiagonalcausalwithoffsetpaddedkeysmask": 4, "blockdiagonalgappykeysmask": 4, "_gappyseqinfo": 4, "gappi": 4, "kv_seqstart": 4, "make_pag": 4, "notional_pad": 4, "paged_typ": 4, "pagedblockdiagonalgappykeysmask": 4, "our": [4, 12], "live": 4, "separ": 4, "convert": 4, "version": [4, 7], "blockdiagonalcausalwithoffsetgappykeysmask": 4, "unlik": 4, "address": 4, "were": 4, "do": [4, 7, 9, 12, 13], "two": [4, 11, 13], "100": 4, "chang": [4, 7], "would": [4, 9, 12, 13], "101": 4, "200": 4, "But": 4, "band": 4, "its": 4, "_subtensor": 4, "attentionbiassubtensor": 4, "lower": 4, "aka": 4, "add_bia": 4, "memory_efficient_attention_parti": 4, "lse": 4, "style": 4, "extra": [4, 9, 11, 12], "data": [4, 13], "log": [4, 9], "sum": 4, "merge_attent": 4, "obtain": 4, "against": [4, 14], "disjoint": 4, "backward": 4, "quit": 4, "restrict": 4, "particular": [4, 12], "weren": 4, "anywher": 4, "attn_split": 4, "lse_split": 4, "write_ls": 4, "get": [4, 11, 12], "whole": 4, "arxiv": 4, "org": 4, "ab": 4, "2402": 4, "05099": 4, "result": 4, "out_ful": 4, "out1": 4, "lse1": 4, "out2": 4, "lse2": 4, "lse_ful": 4, "chunk": [4, 11], "kq": 4, "singl": 4, "num_chunk": 4, "attn_out": 4, "lse_out": 4, "memory_efficient_attention_backward": 4, "grad": 4, "gradient": 4, "dq": 4, "dk": 4, "dv": 4, "explan": 4, "memory_efficient_attention_forward_requires_grad": 4, "memory_efficient_attention_forward": 4, "later": 4, "positional_embed": 5, "rotaryembed": 5, "roform": 5, "su": 5, "crucial": 5, "insight": 5, "rotat": 5, "matric": 5, "rel": 5, "avail": 5, "repo": [5, 14], "gpt": 5, "neox": 5, "inspir": [5, 14], "sinepositionalembed": 5, "positionembed": 5, "vocabembed": 5, "vocab_s": 5, "init_weight": 5, "gain": 5, "build_positional_embed": 5, "positionembeddingconfig": 5, "encod": 5, "my_position_encod": 5, "register_positional_embed": 5, "determinist": 6, "net": 6, "record_rng": 6, "set_rng": 6, "reversibleblock": [6, 11], "split_dim": 6, "f_arg": [6, 11], "g_arg": [6, 11], "backward_pass": 6, "dy": 6, "reversiblesequ": [6, 11], "modulelist": [6, 11], "arg_rout": [6, 11], "transpar": 7, "sputnik": 7, "These": 7, "instal": 7, "recipi": 7, "machin": 7, "compil": 7, "git": 7, "clone": 7, "fairintern": 7, "conda": 7, "xformer_env": 7, "python": 7, "cd": 7, "pip": 7, "txt": 7, "issu": 7, "relat": [7, 11], "nvcc": 7, "current": 7, "runtim": 7, "match": 7, "often": 7, "unload": 7, "xx": 7, "gcc": 7, "re": [7, 12, 13], "torch_cuda_arch_list": 7, "env": 7, "variabl": 7, "architur": 7, "suggest": 7, "setup": 7, "comprehens": 7, "export": 7, "7": 7, "trigger": [7, 9], "enough": 7, "30": 7, "There": [7, 9, 13], "noth": 7, "specif": [7, 14], "tutori": 7, "host": [8, 10], "flexibl": [8, 14], "interoper": [8, 14], "state": [8, 14], "art": [8, 14], "api": [8, 13], "replac": [8, 10], "exist": [8, 10], "vit": [8, 10], "extend": [8, 10, 14], "zoo": [8, 10, 14], "interest": [8, 10], "done": 9, "privat": 9, "fork": 9, "progress": 9, "someth": [9, 11, 13], "share": 9, "point": 9, "directli": [9, 13], "order": 9, "submit": 9, "pr": [9, 14], "practic": [9, 12], "unit": 9, "loos": 9, "inherit": 9, "expos": [9, 11, 12, 13], "exact": 9, "interfac": 9, "let": [9, 12], "consid": [9, 11, 12], "dataclass": 9, "nystromselfattentionconfig": 9, "def": [9, 11, 12], "paper": [9, 11], "remark": 9, "extens": [9, 14], "explicitli": 9, "constructor": 9, "It": 9, "benchmark": [9, 14], "construct": 9, "field": [9, 14], "registr": 9, "snippet": 9, "ti": 9, "open": [9, 11], "up": [9, 11], "least": 9, "tool": 9, "toolbox": 9, "relev": [9, 14], "now": [9, 12], "pick": 9, "variant": [9, 14], "go": 9, "pytest": 9, "my_component_nam": 9, "applic": [9, 11], "lra": 9, "json": 9, "job": 9, "As": 9, "remind": 9, "inform": 9, "dedic": 9, "readm": 9, "python3": 9, "run_task": 9, "py": 9, "task": 9, "config_path": 9, "world_siz": 9, "slurm": 9, "enabl": 9, "cluster": 9, "batch_submit": 9, "checkpo": 9, "gomez": 11, "Its": 11, "context": 11, "reform": 11, "unrel": 11, "lsh": 11, "process": 11, "lightli": 11, "adapt": 11, "robin": 11, "bruegger": 11, "lucidrain": 11, "x1": 11, "x2": 11, "produc": 11, "y1": 11, "y2": 11, "turn": [11, 13], "recov": 11, "detail": 11, "anoth": 11, "fw": 11, "effect": 11, "compar": [11, 14], "checkpoint": 11, "tradeoff": 11, "One": 11, "benefit": 11, "natur": 11, "free": 11, "help": 11, "save": 11, "commun": 11, "cost": 11, "moreov": 11, "stack": 11, "increas": 11, "norm": 11, "close": 11, "formul": 11, "deal": 11, "accuraci": 11, "affect": 11, "verifi": 11, "repositori": [11, 12], "main": 11, "take": [11, 12, 13], "sequenti": 11, "similarli": 11, "rout": 11, "complet": [11, 13], "factori": 11, "model_factori": 11, "yet": 11, "compat": 11, "ddp": 11, "xformerstackconfig": 11, "block_config": 11, "xformerencoderconfig": 11, "xformerdecoderconfig": 11, "num_lay": 11, "ren": 11, "urtasun": 11, "gross": 11, "2017": 11, "network": 11, "backpropag": 11, "store": 11, "kitaev": 11, "kaiser": 11, "\u0142": 11, "levskaya": 11, "sai": 12, "experi": 12, "show": 12, "how": 12, "reus": [12, 14], "imag": 12, "aspect": 12, "translat": 12, "just": 12, "check": 12, "notebook": 12, "exhaust": 12, "timm": 12, "vision_transform": 12, "visiontransform": 12, "timm_sparse_attent": 12, "timmsparseattent": 12, "img_siz": 12, "224": 12, "patch_siz": 12, "embed_dim": 12, "96": 12, "depth": 12, "mlp_ratio": 12, "qkv_bia": 12, "norm_lay": 12, "layernorm": 12, "suppos": 12, "snipper": 12, "precis": 12, "mind": 12, "alreadi": 12, "sever": 12, "attention_pattern": 12, "my_fancy_mask": 12, "recurs": 12, "monkei": 12, "patch": 12, "replace_attn_with_xformers_on": 12, "module_output": 12, "isinst": 12, "qkv": 12, "minim": 12, "attn_mask": 12, "child": 12, "named_children": 12, "add_modul": 12, "del": 12, "variat": 12, "exchang": 12, "mai": 12, "good": 12, "idea": 12, "closer": 12, "exhibit": 12, "clear": 12, "sparsiti": 12, "alter": 12, "sparsifi": 12, "fine": 13, "similar": 13, "own": 13, "everyth": 13, "requires_head_dimens": 13, "flag": 13, "defer": 13, "lot": 13, "littl": 13, "obscur": 13, "although": 13, "hopefulli": 13, "straightforward": 13, "built": 13, "intern": 13, "sure": 13, "programat": 13, "sweep": 13, "search": [13, 14], "multihead": 13, "definit": 13, "seq": 13, "1024": 13, "384": 13, "my_config": 13, "attention_nam": 13, "easili": [13, 14], "rand": 13, "dummi": 13, "multi_head": 13, "my": 13, "focus": 14, "agnost": 14, "design": 14, "ideal": 14, "break": 14, "studi": 14, "ablat": 14, "aim": 14, "easi": 14, "focu": 14, "improv": 14, "across": 14, "domain": 14, "engin": 14, "effort": 14, "And": 14, "sinc": 14, "measur": 14, "heavi": 14, "everi": 14, "alon": 14, "happen": 14, "anytim": 14, "somebodi": 14, "through": 14, "crowd": 14, "realli": 14, "welcom": 14, "move": 14, "too": 14, "anyth": 14}, "objects": {"xformers.components": [[3, 0, 1, "", "MultiHeadDispatch"], [0, 3, 0, "-", "attention"], [1, 3, 0, "-", "feedforward"], [5, 3, 0, "-", "positional_embedding"]], "xformers.components.MultiHeadDispatch": [[3, 1, 1, "", "forward"], [3, 1, 1, "", "from_config"], [3, 2, 1, "", "training"]], "xformers.components.attention": [[0, 0, 1, "", "Attention"], [0, 0, 1, "", "AttentionMask"], [0, 0, 1, "", "FavorAttention"], [0, 0, 1, "", "GlobalAttention"], [0, 0, 1, "", "LinformerAttention"], [0, 0, 1, "", "LocalAttention"], [0, 0, 1, "", "NystromAttention"], [0, 0, 1, "", "OrthoFormerAttention"], [0, 0, 1, "", "RandomAttention"], [0, 0, 1, "", "ScaledDotProduct"], [0, 5, 1, "", "build_attention"], [0, 5, 1, "", "register_attention"]], "xformers.components.attention.Attention": [[0, 1, 1, "", "forward"], [0, 1, 1, "", "from_config"]], "xformers.components.attention.AttentionMask": [[0, 4, 1, "", "device"], [0, 4, 1, "", "dtype"], [0, 1, 1, "", "from_bool"], [0, 1, 1, "", "from_multiplicative"], [0, 4, 1, "", "is_sparse"], [0, 1, 1, "", "make_causal"], [0, 1, 1, "", "make_crop"], [0, 4, 1, "", "ndim"], [0, 4, 1, "", "shape"], [0, 1, 1, "", "to"], [0, 1, 1, "", "to_bool"]], "xformers.components.attention.FavorAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.GlobalAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.LinformerAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.LocalAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.NystromAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.OrthoFormerAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.RandomAttention": [[0, 1, 1, "", "__init__"], [0, 1, 1, "", "forward"]], "xformers.components.attention.ScaledDotProduct": [[0, 1, 1, "", "forward"], [0, 2, 1, "", "mask"]], "xformers.components.feedforward": [[1, 0, 1, "", "Feedforward"], [1, 0, 1, "", "MLP"], [1, 5, 1, "", "build_feedforward"], [1, 5, 1, "", "register_feedforward"]], "xformers.components.feedforward.Feedforward": [[1, 1, 1, "", "from_config"], [1, 2, 1, "", "training"]], "xformers.components.feedforward.MLP": [[1, 1, 1, "", "forward"], [1, 2, 1, "", "training"]], "xformers.components.positional_embedding": [[5, 0, 1, "", "RotaryEmbedding"], [5, 0, 1, "", "SinePositionalEmbedding"], [5, 0, 1, "", "VocabEmbedding"], [5, 5, 1, "", "build_positional_embedding"], [5, 5, 1, "", "register_positional_embedding"]], "xformers.components.positional_embedding.RotaryEmbedding": [[5, 1, 1, "", "forward"], [5, 2, 1, "", "training"]], "xformers.components.positional_embedding.SinePositionalEmbedding": [[5, 1, 1, "", "forward"], [5, 2, 1, "", "training"]], "xformers.components.positional_embedding.VocabEmbedding": [[5, 1, 1, "", "forward"], [5, 1, 1, "", "init_weights"], [5, 2, 1, "", "training"]], "xformers.components.reversible": [[6, 0, 1, "", "Deterministic"], [6, 0, 1, "", "ReversibleBlock"], [6, 0, 1, "", "ReversibleSequence"]], "xformers.components.reversible.Deterministic": [[6, 1, 1, "", "forward"], [6, 1, 1, "", "record_rng"], [6, 2, 1, "", "training"]], "xformers.components.reversible.ReversibleBlock": [[6, 1, 1, "", "backward_pass"], [6, 1, 1, "", "forward"], [6, 2, 1, "", "training"]], "xformers.components.reversible.ReversibleSequence": [[6, 1, 1, "", "forward"], [6, 2, 1, "", "training"]], "xformers": [[4, 3, 0, "-", "ops"]], "xformers.ops": [[4, 0, 1, "", "AttentionOpBase"], [4, 3, 0, "module-0", "fmha"], [4, 5, 1, "", "memory_efficient_attention"]], "xformers.ops.AttentionOpBase": [[4, 1, 1, "", "not_supported_reasons"]], "xformers.ops.fmha": [[4, 3, 0, "-", "attn_bias"], [4, 3, 0, "-", "ck"], [4, 3, 0, "-", "ck_decoder"], [4, 3, 0, "-", "ck_splitk"], [4, 3, 0, "-", "cutlass"], [4, 3, 0, "-", "flash"], [4, 5, 1, "", "memory_efficient_attention_backward"], [4, 5, 1, "", "memory_efficient_attention_forward"], [4, 5, 1, "", "memory_efficient_attention_forward_requires_grad"], [4, 5, 1, "", "memory_efficient_attention_partial"], [4, 5, 1, "", "merge_attentions"]], "xformers.ops.fmha.attn_bias": [[4, 0, 1, "", "AttentionBias"], [4, 0, 1, "", "BlockDiagonalCausalFromBottomRightMask"], [4, 0, 1, "", "BlockDiagonalCausalLocalAttentionFromBottomRightMask"], [4, 0, 1, "", "BlockDiagonalCausalLocalAttentionMask"], [4, 0, 1, "", "BlockDiagonalCausalLocalAttentionPaddedKeysMask"], [4, 0, 1, "", "BlockDiagonalCausalMask"], [4, 0, 1, "", "BlockDiagonalCausalWithOffsetGappyKeysMask"], [4, 0, 1, "", "BlockDiagonalCausalWithOffsetPaddedKeysMask"], [4, 0, 1, "", "BlockDiagonalGappyKeysMask"], [4, 0, 1, "", "BlockDiagonalMask"], [4, 0, 1, "", "BlockDiagonalPaddedKeysMask"], [4, 0, 1, "", "LocalAttentionFromBottomRightMask"], [4, 0, 1, "", "LowerTriangularFromBottomRightLocalAttentionMask"], [4, 0, 1, "", "LowerTriangularFromBottomRightMask"], [4, 0, 1, "", "LowerTriangularMask"], [4, 0, 1, "", "LowerTriangularMaskWithTensorBias"], [4, 0, 1, "", "PagedBlockDiagonalCausalWithOffsetPaddedKeysMask"], [4, 0, 1, "", "PagedBlockDiagonalGappyKeysMask"], [4, 0, 1, "", "PagedBlockDiagonalPaddedKeysMask"]], "xformers.ops.fmha.attn_bias.AttentionBias": [[4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetGappyKeysMask": [[4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalCausalWithOffsetPaddedKeysMask": [[4, 1, 1, "", "from_seqlens"]], "xformers.ops.fmha.attn_bias.BlockDiagonalGappyKeysMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "make_paged"], [4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.BlockDiagonalMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "from_tensor_list"], [4, 1, 1, "", "make_causal"], [4, 1, 1, "", "make_causal_from_bottomright"], [4, 1, 1, "", "make_local_attention"], [4, 1, 1, "", "make_local_attention_from_bottomright"], [4, 1, 1, "", "materialize"], [4, 1, 1, "", "split"]], "xformers.ops.fmha.attn_bias.BlockDiagonalPaddedKeysMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.LowerTriangularFromBottomRightMask": [[4, 1, 1, "", "make_local_attention"]], "xformers.ops.fmha.attn_bias.LowerTriangularMask": [[4, 1, 1, "", "add_bias"]], "xformers.ops.fmha.attn_bias.PagedBlockDiagonalGappyKeysMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "materialize"]], "xformers.ops.fmha.attn_bias.PagedBlockDiagonalPaddedKeysMask": [[4, 1, 1, "", "from_seqlens"], [4, 1, 1, "", "materialize"]], "xformers.ops.fmha.ck": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.ck_decoder": [[4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.cutlass": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]], "xformers.ops.fmha.flash": [[4, 0, 1, "", "BwOp"], [4, 0, 1, "", "FwOp"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:attribute", "3": "py:module", "4": "py:property", "5": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "attribute", "Python attribute"], "3": ["py", "module", "Python module"], "4": ["py", "property", "Python property"], "5": ["py", "function", "Python function"]}, "titleterms": {"attent": [0, 3, 4, 12, 13], "mechan": [0, 1, 13], "feedforward": 1, "api": 2, "refer": [2, 7], "multi": 3, "head": 3, "xformer": [4, 8, 9, 14], "optim": 4, "oper": 4, "memori": 4, "effici": 4, "avail": 4, "implement": 4, "bias": 4, "partial": 4, "non": 4, "autograd": 4, "posit": 5, "embed": 5, "revers": [6, 11], "layer": 6, "custom": [7, 8], "part": [7, 8, 9], "spars": [7, 12], "cuda": 7, "kernel": 7, "1": 7, "build": 7, "2": 7, "usag": 7, "welcom": 8, "s": 8, "document": 8, "compon": 8, "tutori": [8, 10], "exampl": 8, "some": 8, "extend": 9, "zoo": 9, "us": 11, "block": 11, "intro": 11, "transform": 11, "In": 11, "practic": 11, "replac": 12, "all": 12, "from": 12, "an": 12, "exist": 12, "vit": 12, "model": 12, "equival": 12, "i": 13, "m": 13, "onli": 13, "interest": 13, "test": 13, "out": 13, "ar": 13, "host": 13, "here": 13, "what": 14}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 6, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinx": 56}})